{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb19ba2",
   "metadata": {},
   "source": [
    "# Using Gambit with OpenSpiel\n",
    "\n",
    "This tutorial demonstrates the interoperability of the Gambit and OpenSpiel Python packages for game-theoretic analysis.\n",
    "\n",
    "Gambit provides a range of methods to compute exact and close approximations of equilibria for games. OpenSpiel provides a variety of iterative multi-agent learning algorithms, which may or may not converge to equilibria.\n",
    "\n",
    "Another key distinction is that the PyGambit API allows the user a simple way to define custom games (see tutorials 1-3). This is also possible in OpenSpiel for normal-form games, and you can load `.efg` files created from Gambit for the extensive-form, however some of the key functionality for iterated learning of strategies is only available for games from the built-in library (see the [OpenSpiel documentation](https://openspiel.readthedocs.io/en/latest/games.html)).\n",
    "\n",
    "This tutorial demonstrates:\n",
    "\n",
    "1. Transferring examples of normal (strategic) form and extensive-form games between OpenSpiel and Gambit\n",
    "2. Simulating evolutionary dynamics of populations of strategies in OpenSpiel for normal-form games\n",
    "3. Training agents using self-play of extensive-form games in OpenSpiel to create strategies\n",
    "4. Comparing the strategies from OpenSpiel against equilibrium strategies computed with Gambit\n",
    "\n",
    "Note: The OpenSpiel code was adapted from the introductory tutorial for the OpenSpiel API on colab [here](https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/OpenSpielTutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb78322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "from open_spiel.python.algorithms.gambit import export_gambit\n",
    "from open_spiel.python.egt import dynamics\n",
    "from open_spiel.python.egt.utils import game_payoffs_array\n",
    "\n",
    "import pygambit as gbt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd324814",
   "metadata": {},
   "source": [
    "## OpenSpiel game library\n",
    "\n",
    "OpenSpiel has a large selection of games available in its [library](https://openspiel.readthedocs.io/en/latest/games.html). Many of these will not be amenable to equilibrium computation with Gambit, due to their size. For the purposes of this tutorial, we'll pick some of the smallest games from the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyspiel.registered_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e628a86d",
   "metadata": {},
   "source": [
    "## Normal-form games from the OpenSpiel library\n",
    "\n",
    "Let's start with the simple normal-form game of rock-paper-scissors, in which the payoffs can be represented by a 3x3 matrix.\n",
    "\n",
    "Load matrix rock-paper-scissors from OpenSpiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_matrix_rps_game = pyspiel.load_game(\"matrix_rps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1204e",
   "metadata": {},
   "source": [
    "In order to simulate a playthrough of the game, you can first initialise a game state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ops_matrix_rps_game.new_initial_state()\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee015a",
   "metadata": {},
   "source": [
    "The possible actions for both players (player 0 and player 1) are Rock, Paper and Scissors, but these are not labelled and must be accessed via integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70575dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.legal_actions(0)) # Player 0 (row) actions\n",
    "print(state.legal_actions(1)) # Player 1 (column) actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea7e5b",
   "metadata": {},
   "source": [
    "Since Rock-paper-scissors is a 1-step simultaneous-move normal-form game, we'll apply a list of player actions in one step to reach the terminal state.\n",
    "\n",
    "Let's simulate player 0 playing Rock (0) and player 1 playing Paper (1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.apply_actions([0, 1])\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cf8dd",
   "metadata": {},
   "source": [
    "OpenSpiel can generate an NFG representation of the game loadable in Gambit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfg_matrix_rps_game = pyspiel.game_to_nfg_string(ops_matrix_rps_game)\n",
    "nfg_matrix_rps_game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1df64",
   "metadata": {},
   "source": [
    "Now let's load the NFG in Gambit. Since Gambit's `read_nfg` function expects a file like object, we'll convert the string with `io.StringIO`.\n",
    "We can also add labels for the actions to make the output more interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_matrix_rps_game = gbt.read_nfg(StringIO(nfg_matrix_rps_game))\n",
    "\n",
    "gbt_matrix_rps_game.title = \"Rock-Paper-Scissors\"\n",
    "\n",
    "for player in gbt_matrix_rps_game.players:\n",
    "    player.strategies[0].label = \"Rock\"\n",
    "    player.strategies[1].label = \"Paper\"\n",
    "    player.strategies[2].label = \"Scissors\"\n",
    "\n",
    "gbt_matrix_rps_game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7da6f3",
   "metadata": {},
   "source": [
    "The unique equilibrium mixed strategy profile for both players is to choose rock, paper, and scissors with equal probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt.nash.lcp_solve(gbt_matrix_rps_game).equilibria[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e7e3f",
   "metadata": {},
   "source": [
    "We can use OpenSpiel's dynamics module to demonstrate evolutionary game theory dynamics, or \"replicator dynamics\", which models how a mixed strategy profile evolves over time based on how the strategies (e.g., choice of actions A, B, C with probabilities X, Y, Z) perform against one another.\n",
    "\n",
    "Let's start with an initial profile that is not at equilibrium, but weighted towards scissors with proportions: 30% Rock, 30% Paper, 40% Scissors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1acdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rps_payoffs = game_payoffs_array(ops_matrix_rps_game)\n",
    "dyn = dynamics.SinglePopulationDynamics(matrix_rps_payoffs, dynamics.replicator)\n",
    "x = np.array([0.3, 0.3, 0.4])\n",
    "dyn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa382753",
   "metadata": {},
   "source": [
    "`dyn(x)` calculates the rate of change (derivative) for each strategy in the current profile and returns how fast each strategy's frequency is changing.\n",
    "\n",
    "In replicator dynamics, a pure strategy that performs well against others will increase in frequency, while strategies performing worse will decrease.\n",
    "In our rock-paper-scissors example, the performance of each pure strategy (action) depends on the probability it is assigned in the mixed strategy profile. At the start, whilst there are more players choosing scissors as their action, then rock will perform well and increase in frequency (be more likely to get played in subsequent rounds), while paper will perform poorly and decrease in frequency. We can plot how the frequency of each strategy changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a352c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rps_dynamics(proportions, steps=100, alpha=0.1, plot_average_strategy=False):\n",
    "    x = np.array(proportions)\n",
    "    rock_proportions = [x[0]]\n",
    "    paper_proportions = [x[1]]\n",
    "    scissors_proportions = [x[2]]\n",
    "    y = []\n",
    "    for _ in range(steps):\n",
    "        x += alpha * dyn(x)\n",
    "        rock_proportions.append(x[0])\n",
    "        paper_proportions.append(x[1])\n",
    "        scissors_proportions.append(x[2])\n",
    "        if plot_average_strategy:\n",
    "            y.append([np.mean(rock_proportions),\n",
    "                      np.mean(paper_proportions),\n",
    "                      np.mean(scissors_proportions)\n",
    "                      ])\n",
    "        else:\n",
    "            y.append(x.copy())\n",
    "    y = np.array(y)\n",
    "\n",
    "    plt.plot(y[:, 0], label=\"Rock\")\n",
    "    plt.plot(y[:, 1], label=\"Paper\")\n",
    "    plt.plot(y[:, 2], label=\"Scissors\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    if plot_average_strategy:\n",
    "        plt.ylabel(\"Strategy frequency average up to time step\")\n",
    "    else:\n",
    "        plt.ylabel(\"Strategy frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_rps_dynamics([0.3, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569aef4",
   "metadata": {},
   "source": [
    "Through the dynamics, we can see that the population proportions oscillate around the equilibrium point (1/3, 1/3, 1/3) without converging to it, because the best strategy depends on the likelihood of the opponents' actions, as defined by the current action probabilities.\n",
    "\n",
    "However, if we start with the initial population already at the equilibrium mixed strategy profile computed by Gambit (each action is chosen exactly 1/3 of the time), the strategy frequencies will remain constant over time (at the equilibrium point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rps_dynamics([1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6662e",
   "metadata": {},
   "source": [
    "When starting from an unbalanced initial mixed strategy profile, the strategy frequencies will oscillate around the equilibrium point without converging to it. However, if we plot the average strategy frequencies over time, we can see that this begins to converge to the equilibrium point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rps_dynamics([0.3, 0.3, 0.4], plot_average_strategy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a21e0",
   "metadata": {},
   "source": [
    "## Normal-form games created with Gambit\n",
    "\n",
    "You can also set up a normal-form game in Gambit and export it to OpenSpiel. Here we demonstrate this with the simple Prisoner's Dilemma game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "player1_payoffs = np.array([[-1, -3], [0, -2]])\n",
    "player2_payoffs = np.transpose(player1_payoffs)\n",
    "\n",
    "gbt_prisoners_dilemma_game = gbt.Game.from_arrays(\n",
    "    player1_payoffs,\n",
    "    player2_payoffs,\n",
    "    title=\"Prisoner's Dilemma\"\n",
    ")\n",
    "gbt_prisoners_dilemma_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt.nash.lcp_solve(gbt_prisoners_dilemma_game).equilibria[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd432d",
   "metadata": {},
   "source": [
    "As expected, Gambit computes the unique equilibrium strategy for both players as choosing cooperate with probability 0 and defect with probability 1.\n",
    "\n",
    "To re-create the game in OpenSpiel we extract the player payoffs to NumPy arrays, which are then used to create a matrix game in OpenSpiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd42af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_payoffs, p2_payoffs = gbt_prisoners_dilemma_game.to_arrays(dtype=float)\n",
    "ops_prisoners_dilemma_game = pyspiel.create_matrix_game(\n",
    "    gbt_prisoners_dilemma_game.title,\n",
    "    \"Classic Prisoner's Dilemma\",  # description\n",
    "    [strategy.label for strategy in gbt_prisoners_dilemma_game.players[0].strategies],\n",
    "    [strategy.label for strategy in gbt_prisoners_dilemma_game.players[1].strategies],\n",
    "    p1_payoffs,\n",
    "    p2_payoffs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a35a4",
   "metadata": {},
   "source": [
    "Like rock-paper-scissors, the Prisoner's Dilemma is a 1-step simultaneous-move normal-form game; we'll apply a list of player actions in one step to reach the terminal state. Let's have both player choose to defect (1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ops_prisoners_dilemma_game.new_initial_state()\n",
    "state.apply_actions([1, 1])\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea0224",
   "metadata": {},
   "source": [
    "Unlike in rock-paper-scissors, the Prisoner's Dilemma has a dominant strategy equilibrium, in which both players defect.\n",
    "Using evolutionary dynamics, we can see that a population starting with a mix of cooperators and defectors will evolve towards all defectors over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1495c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_pd_payoffs = game_payoffs_array(ops_prisoners_dilemma_game)\n",
    "pd_dyn = dynamics.SinglePopulationDynamics(matrix_pd_payoffs, dynamics.replicator)\n",
    "\n",
    "def plot_pd_dynamics(proportions, steps=100, alpha=0.1):\n",
    "    x = np.array(proportions)\n",
    "    y = []\n",
    "    for _ in range(steps):\n",
    "        x += alpha * pd_dyn(x)\n",
    "        y.append(x.copy())\n",
    "    y = np.array(y)\n",
    "    plt.plot(y[:, 0], label=\"Cooperate\")\n",
    "    plt.plot(y[:, 1], label=\"Defect\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_pd_dynamics([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926fb07",
   "metadata": {},
   "source": [
    "<!-- ## Extensive-form example: Silly1111 Poker -->\n",
    "\n",
    "<!-- Silly poker is a variant imperfect information one-card poker game introduced in tutorial 3, but in which there are 3 possible cards (J, Q, K) instead of 2. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f6330",
   "metadata": {},
   "source": [
    "## Extensive-form games from the OpenSpiel library\n",
    "\n",
    "For extensive-form games, OpenSpiel can export to the EFG format used by Gambit. Here we demonstrate this with **Tiny Hanabi**, loaded from the OpenSpiel [game library](https://openspiel.readthedocs.io/en/latest/games.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a42600",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_hanabi_game = pyspiel.load_game(\"tiny_hanabi\")\n",
    "efg_hanabi_game = export_gambit(ops_hanabi_game)\n",
    "efg_hanabi_game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa354c9f",
   "metadata": {},
   "source": [
    "Now let's load the EFG in Gambit.\n",
    "We can then compute equilibria strategies for the players as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a534e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_hanabi_game = gbt.read_efg(StringIO(efg_hanabi_game))\n",
    "eqm = gbt.nash.lcp_solve(gbt_hanabi_game).equilibria[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_tree import draw_tree\n",
    "\n",
    "draw_tree(\n",
    "    gbt_hanabi_game,\n",
    "    color_scheme=\"gambit\",\n",
    "    edge_thickness=2,\n",
    "    action_label_position=0.8,\n",
    "    shared_terminal_depth=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe924e",
   "metadata": {},
   "source": [
    "We can look at player 0's equilibrium strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec19b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqm[\"Pl0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54411c0",
   "metadata": {},
   "source": [
    "...and use Gambit to explore what those numbers actually mean for player 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for infoset, mixed_action in eqm[\"Pl0\"].mixed_actions():\n",
    "    print(\n",
    "        f\"At information set {infoset.number}, \"\n",
    "        f\"Player 0 plays action 0 with probability: {mixed_action['p0a0']}\"\n",
    "        f\" and action 1 with probability: {mixed_action['p0a1']}\"\n",
    "        f\" and action 2 with probability: {mixed_action['p0a2']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac73a24",
   "metadata": {},
   "source": [
    "For player 1, we can do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqm[\"Pl1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for infoset, mixed_action in eqm[\"Pl1\"].mixed_actions():\n",
    "    print(\n",
    "        f\"At information set {infoset.number}, \"\n",
    "        f\"Player 1 plays action 0 with probability: {mixed_action['p1a0']}\"\n",
    "        f\" and action 1 with probability: {mixed_action['p1a1']}\"\n",
    "        f\" and action 2 with probability: {mixed_action['p1a2']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c0d5",
   "metadata": {},
   "source": [
    "Let's now train 2 agents using independent Q-learning on Tiny Hanabi, and play them against eachother.\n",
    "\n",
    "We can compare the learned strategies played to the equilibrium strategies computed by Gambit.\n",
    "\n",
    "First let's open the RL environment for Tiny Hanabi and create the agents, one for each player (2 players in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = rl_environment.Environment(\"tiny_hanabi\")\n",
    "num_players = env.num_players\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "# Create the agents\n",
    "agents = [\n",
    "    tabular_qlearner.QLearner(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9eea4",
   "metadata": {},
   "source": [
    "Now we can train the Q-learning agents in self-play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53547263",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_episode in range(30000):\n",
    "  if cur_episode % 10000 == 0:\n",
    "    print(f\"Episodes: {cur_episode}\")\n",
    "\n",
    "  time_step = env.reset()\n",
    "  while not time_step.last():\n",
    "    player_id = time_step.observations[\"current_player\"]\n",
    "    agent_output = agents[player_id].step(time_step)\n",
    "    time_step = env.step([agent_output.action])\n",
    "\n",
    "  # Episode is over, step all agents with final info state.\n",
    "  for agent in agents:\n",
    "    agent.step(time_step)\n",
    "\n",
    "print(f\"Episodes: {cur_episode+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cddd36",
   "metadata": {},
   "source": [
    "Let's check out the strategies our agents have learned by playing them against eachother again, this time in evaluation mode (setting `is_evaluation=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = env.reset()\n",
    "\n",
    "while not time_step.last():\n",
    "  print(\"\")\n",
    "  print(env.get_state)\n",
    "\n",
    "  player_id = time_step.observations[\"current_player\"]\n",
    "  agent_output = agents[player_id].step(time_step, is_evaluation=True)\n",
    "  print(f\"Agent {player_id} chooses {env.get_state.action_to_string(agent_output.action)}\")\n",
    "  time_step = env.step([agent_output.action])\n",
    "\n",
    "print(\"\")\n",
    "print(env.get_state)\n",
    "print(f\"Rewards: {time_step.rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9b174",
   "metadata": {},
   "source": [
    "Are the learned strategies chosen by p0 and p1 consistent with an equilibrium computed by Gambit?\n",
    "\n",
    "When I ran the above I got the final game state `p0:d0 p1:d0 p0:a2 p1:a0` with payoffs `[10.0, 10.0]`. This is consistent with the equilibrium computed by Gambit:\n",
    "- The node `p0:d0 p1:d0` is part of player 0's information set 0.\n",
    "- p0 picks a2 which matches the first equilibrium strategy in `eqm['Pl0']` where action `p0a2` is played with probability 1.0.\n",
    "- This puts player 1 in their information set 2, and player 1 picks action 0, which is consistent with `eqm['Pl1']` where action `p1a0` is played with probability 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f356383",
   "metadata": {},
   "source": [
    "## Extensive-form games created with Gambit\n",
    "\n",
    "It's also possible to create an extensive-form game in Gambit and export it to OpenSpiel. Here we demonstrate this with the one-card poker game introduced in tutorial 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_one_card_poker = gbt.Game.new_tree(\n",
    "    players=[\"Alice\", \"Bob\"],\n",
    "    title=\"Stripped-Down Poker: a simple game of one-card poker from Reiley et al (2008).\"\n",
    ")\n",
    "\n",
    "gbt_one_card_poker.append_move(\n",
    "    gbt_one_card_poker.root,\n",
    "    player=gbt_one_card_poker.players.chance,\n",
    "    actions=[\"King\", \"Queen\"]  # By default, chance actions have equal probabilities\n",
    ")\n",
    "\n",
    "for node in gbt_one_card_poker.root.children:\n",
    "    gbt_one_card_poker.append_move(\n",
    "        node,\n",
    "        player=\"Alice\",\n",
    "        actions=[\"Bet\", \"Fold\"]\n",
    "    )\n",
    "\n",
    "gbt_one_card_poker.append_move(\n",
    "    [\n",
    "        gbt_one_card_poker.root.children[\"King\"].children[\"Bet\"],\n",
    "        gbt_one_card_poker.root.children[\"Queen\"].children[\"Bet\"]\n",
    "    ],\n",
    "    player=\"Bob\",\n",
    "    actions=[\"Call\", \"Fold\"]\n",
    ")\n",
    "\n",
    "win_big = gbt_one_card_poker.add_outcome([2, -2], label=\"Win Big\")\n",
    "win = gbt_one_card_poker.add_outcome([1, -1], label=\"Win\")\n",
    "lose_big = gbt_one_card_poker.add_outcome([-2, 2], label=\"Lose Big\")\n",
    "lose = gbt_one_card_poker.add_outcome([-1, 1], label=\"Lose\")\n",
    "\n",
    "# Alice folds, Bob wins small\n",
    "gbt_one_card_poker.set_outcome(\n",
    "    gbt_one_card_poker.root.children[\"King\"].children[\"Fold\"],\n",
    "    lose\n",
    ")\n",
    "gbt_one_card_poker.set_outcome(\n",
    "    gbt_one_card_poker.root.children[\"Queen\"].children[\"Fold\"],\n",
    "    lose\n",
    ")\n",
    "\n",
    "# Bob sees Alice Bet and calls, correctly believing she is bluffing, Bob wins big\n",
    "gbt_one_card_poker.set_outcome(\n",
    "    gbt_one_card_poker.root.children[\"Queen\"].children[\"Bet\"].children[\"Call\"],\n",
    "    lose_big\n",
    ")\n",
    "\n",
    "# Bob sees Alice Bet and calls, incorrectly believing she is bluffing, Alice wins big\n",
    "gbt_one_card_poker.set_outcome(\n",
    "    gbt_one_card_poker.root.children[\"King\"].children[\"Bet\"].children[\"Call\"],\n",
    "    win_big\n",
    ")\n",
    "\n",
    "# Bob does not call Alice's Bet, Alice wins small\n",
    "gbt_one_card_poker.set_outcome(\n",
    "    gbt_one_card_poker.root.children[\"King\"].children[\"Bet\"].children[\"Fold\"],\n",
    "    win\n",
    ")\n",
    "gbt_one_card_poker.set_outcome(\n",
    "    gbt_one_card_poker.root.children[\"Queen\"].children[\"Bet\"].children[\"Fold\"],\n",
    "    win\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed920d33-b7c6-4cc1-b055-7244a5bf42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(gbt_one_card_poker, color_scheme=\"gambit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f296f44",
   "metadata": {},
   "source": [
    "Create the game in OpenSpiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07340e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_one_card_poker = pyspiel.load_efg_game(gbt_one_card_poker.to_efg())\n",
    "ops_one_card_poker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6939f6",
   "metadata": {},
   "source": [
    "Games loaded from EFG in OpenSpiel do not take advantage of the full functionality of the package, for example, it is not possible to carry out training with RL algorithms on these games, as in the example above with Tiny Hanabi. The OpenSpiel documentation explains [how to submit new games to the library](https://openspiel.readthedocs.io/en/latest/developer_guide.html#adding-a-game) if you wish to add your own games.\n",
    "\n",
    "We can however use the state representation and play through the game step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_one_card_poker.num_distinct_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986860c",
   "metadata": {},
   "source": [
    "The one-card poker game has 4 distinct actions, 2 are for the first player (Alice in the example game): \"Bet\" and \"Fold\", and 2 for the second player (Bob): \"Call\" and \"Fold\".\n",
    "\n",
    "Initialising the game state, we can see the current player at the start is the chance player, who deals the cards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ops_one_card_poker.new_initial_state()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23df723",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.legal_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0959f9",
   "metadata": {},
   "source": [
    "Let's make the chance player's action dealing a King (action 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.apply_action(0)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be557706",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.legal_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4291f07",
   "metadata": {},
   "source": [
    "As expected, it's now the first player's (Alice's) turn.\n",
    "Let's have Alice choose to \"Bet\" (action 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.apply_action(0)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63f7d7",
   "metadata": {},
   "source": [
    "As expected, the current player is now player 2 (Bob), let's check the legal actions available to Bob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.legal_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5194f",
   "metadata": {},
   "source": [
    "Player 2 (Bob) now has the option to \"Call\" (action 1) or \"Fold\" (action 2).\n",
    "Let's have Bob choose to \"Fold\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97913fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.apply_action(2)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf09576",
   "metadata": {},
   "source": [
    "Since Bob Folded, Alice takes the small win and we reach a terminal state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
